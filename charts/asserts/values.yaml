nameOverride: ""
fullnameOverride: ""
clusterDomain: svc.cluster.local

rbac:
  ## Namespaced role and rolebinding
  ## to allow for get/list/watch of
  ## pod, endpoints, and services in the
  ## installed {{ .Release.Namespace }}
  create: true
  annotations: {}
  extraLabels: {}

## ServiceAccount configuration
##
serviceAccount:
  create: true
  ## name of the service account to use.
  ## note that the values in this file assume all the service
  ## accounts for dependent charts are using the "asserts"
  ## service account. This is here for completness and potential
  ## future changes.
  name: ""
  imagePullSecrets: []
  annotations: {}
  extraLabels: {}

## Assert prometheus endpoints configuration
## This is a list of various prometheus endpoints to query
##
prometheusEndpoints: []
# prometheusEndpoints:
#   - url: prometheus-server0.default.svc.cluster.local:9090
#     scheme: http
#     # recommended
#     env: dev
#   - url: prometheus-external.domain.com
#     scheme: http
#     env: stage
#     # optional
#     site: eu-west-1

## Asserts cluster env and site
## This should be set to the env and site
## of the cluster asserts is running in.
## It should correspond to the env and site of
## one of the configured prometheusEndpoints
## in the above configuration. So if one of the
## endpoints is:
##
# prometheusEndpoints:
#   - url: prometheus-server0.default.svc.cluster.local:9090
#     scheme: http
#     env: dev
#
## then you should set -> assertsClusterEnv: dev
##
## similarly, if connecting to multiple clusters
## with an endpoints list like so:
##
# prometheusEndpoints:
#   - url: prometheus-server0.default.svc.cluster.local:9090
#     scheme: http
#     env: dev
#     site: us-west-2
#   - url: prometheus-server1-external.domain.com
#     scheme: http
#     env: stage
#     site: eu-west-1
#
## but your asserts installation is in the first cluster in the list
## then you should set -> assertsClusterEnv: dev
## and                 -> assertsClusterSite: us-west-2
assertsClusterEnv: ""
assertsClusterSite: ""

## Asserts Url
##
## Used to configure where your notifications
## will link back to in Asserts (see /config/alertmanager-configmap.yaml )
## If setting ui.ingress.enabled: true (similarly a node ip if using NodePort)
## should match the hostname used in addtion to
## adding the desired scheme (http/https)
##
## If unset, leave the url like this as opposed to empty
## as it will work when using port-forwarding.
assertsUrl: http://localhost:8080

## Receiver configuration
## the notification receiver (e.g. slack, pagerduty, etc..)
## wait times/intervals
receiver:
  group_wait: 1m
  group_interval: 15m
  repeat_interval: 30m

## Slack configuration
## template is generated from alertmanager-templates/_slack.tpl
## when slack is enabled
##
slack:
  enabled: false
  # api_url: https://hooks.slack.com/some-url
  # channel: "#alerts"

## PagerDuty configuration
## template is generated from alertmanager-templates/_pagerduty.tpl
## when pagerduty is enabled
##
## ref: https://www.pagerduty.com/docs/guides/prometheus-integration-guide/
pagerduty:
  enabled: false
  # routing_key: <your-events-api-v2-integration-key>
  # url: https://events.pagerduty.com/v2/enqueue

## Asserts server configuration
##
server:
  nameOverride: ""
  fullnameOverride: ""

  image:
    repository: asserts/platform
    pullPolicy: IfNotPresent
    ## Overrides the image tag whose default is the chart appVersion.
    tag: v0.2.141

  initContainers:
    - name: wait-for-postgres
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Values.postgres.fullnameOverride}}.{{.Release.Namespace}}.{{.Values.clusterDomain}}:5432"
        - "-t"
        - "300"

  imagePullSecrets: []

  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}

  service:
    type: ClusterIP
    port: 8030

  resources: {}

  ## wait 30 seconds before pod termination
  ## to allow application shut down
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle
  terminationGracePeriodSeconds: 30

  ## environment variables to add to the asserts-server pod
  extraEnv: []

  ## environment variables from secrets or configmaps to add to the asserts-server pod
  extraEnvFrom: []

  annotations: {}

  extraLabels: {}

  extraPodLabels: {}

  extraPodAnnotations: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  extraContainers: []

  extraVolumeMounts: []

  extraVolumes: []

  dataPath: /opt/asserts/data

  persistence:
    enabled: true

    ## Persistent Volume storage class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
    storageClass: ""

    ## Persistent Volume access modes
    accessModes:
      - ReadWriteOnce

    ## Persistent Volume size
    size: 8Gi

    ## When set, will use the existing PVC for persistence
    existingClaim: ""


## Asserts ui configuration
##
ui:
  nameOverride: ""
  fullnameOverride: ""

  image:
    repository: asserts/asserts-ui
    pullPolicy: IfNotPresent
    ## Overrides the image tag whose default is the chart appVersion.
    tag: v0.1954

  imagePullSecrets: []

  service:
    type: ClusterIP
    port: 8080
    annotations: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  ingress:
    enabled: false

    ## For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}

    extraLabels: {}

    hosts: []
    #   - asserts-ui.domain.com

    path: /

    ## pathType is only for k8s >= 1.18
    pathType: Prefix

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    tls: []
    #   - secretName: asserts-ui-tls
    #     hosts:
    #       - asserts-ui.domain.com

  resources: {}

  annotations: {}

  ## environment variables to add to the asserts-ui pod
  extraEnv: []
  ## environment variables from secrets or configmaps to add to the asserts-ui pod
  extraEnvFrom: []
  #   - secretRef:
  #       name: license

  extraLabels: {}

  extraPodLabels: {}

  podAnnotations: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  extraContainers: []

  extraVolumeMounts: []

  extraVolumes: []


## Asserts grafana configuration
##
grafana:
  nameOverride: ""
  fullnameOverride: ""

  image:
    repository: asserts/grafana
    pullPolicy: IfNotPresent
    ## Overrides the image tag whose default is the chart appVersion.
    tag: v1.0.85

  imagePullSecrets: []

  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}

  service:
    type: ClusterIP
    port: 3000
    annotations: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  ingress:
    enabled: false

    ## For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}

    extraLabels: {}

    hosts: []
    #   - asserts-grafana.domain.com

    path: /

    ## pathType is only for k8s >= 1.18
    pathType: Prefix

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    tls: []
    #   - secretName: asserts-grafana-tls
    #     hosts:
    #       - asserts-grafana.domain.com

  securityContext:
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001

  resources: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  ## Grafana admin password configuration
  ##
  ## If left with password: "" and existingSecret: ""
  ## a random alpha numeric password will be generated.
  ## Upon upgrading the release, you will need to provide
  ## the password as a value in subsequent updates.
  ## e.g.
  # grafana:
  #   auth:
  #     password: <grafana-admin-password>
  ##
  ## If an existingSecret is created (recommended for production)
  ## then that will be used.
  ##
  ## ref: https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues/#credential-errors-while-upgrading-chart-releases
  auth:
    password: ""
    existingSecret: ""

  datasources:
    datasources:
      - name: Prometheus
        url: http://{{.Release.Name}}-promxy.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8082
        isDefault: true
        orgId: 1
        access: proxy
        type: prometheus

  dataPath: /var/lib/grafana/data

  persistence:
    enabled: true

    ## Persistent Volume storage class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
    storageClass: ""

    ## Persistent Volume access modes
    accessModes:
      - ReadWriteOnce

    ## Persistent Volume size
    size: 8Gi

    ## When set, will use the existing PVC for persistence
    existingClaim: ""

  ## 'volumePermissions' init container parameters
  ## Changes the owner and group of the persistent volume mount point to runAsUser:fsGroup values
  ##   based on the *podSecurityContext/*containerSecurityContext parameters
  volumePermissions:
    enabled: true
    image:
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r378
      pullPolicy: IfNotPresent
    resources: {}

## Asserts knowledge-sensor configuration
##
knowledge-sensor:
  image:
    repository: asserts/knowledge-sensor
    tag: v1.1.6

  serviceAccount:
    create: false
    name: asserts

  initContainers:
    - name: wait-for-rules-api
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Release.Name}}-server.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8030"
        - "-t"
        - "300"

  ## tenant and asserts host knowledge sensor is retrieving rules for
  assertsTenant: bootstrap
  assertsControllerHost: "http://{{.Release.Name}}-server.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8030"

  ## how often the knowledge sensor will check for new rules
  syncInterval: "60"

  ## The configmap names and target directories for the rules
  ## and relabel rules the configmaps will contain
  ## NOTE: these should not need to change
  prometheusRulesConfigmapName: "asserts-rules"
  prometheusRulesTargetDir: "/etc/asserts/rules"
  prometheusRelabelRulesConfigmapName: "asserts-relabel-rules"
  prometheusRelabelRulesTargetDir: /etc/asserts/relabel


## Tsdb configuration
## ref: https://github.com/asserts/helm-charts/blob/master/charts/victoria-metrics-single/values.yaml
tsdb:
  enabled: true
  rbac:
    create: false
    pspEnabled: false

  serviceAccount:
    create: false
    name: asserts

  configMap: asserts-tsdb-scrapeconfig
  server:
    image:
      tag: v1.75.1

    nameOverride: "tsdb"
    fullNameOverride: "tsdb"

    initContainers:
      ## wait for the knowledge-sensor to be ready to
      ## serve up the rules
      - name: wait-for-knowledge-sensor
        image: asserts/wait-for:v2.2.3
        imagePullPolicy: IfNotPresent
        args:
          - "{{.Release.Name}}-knowledge-sensor.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8080"
          - "-t"
          - "300"
      ## initially add rule files to volume on startup
      ## after the knowledge-sensor is ready. This is a
      ## way to make sure the tsdb doesn't startup with
      ## empty rules as it will crash or run with an empty
      ## set.
      - name: init-add-rule-files
        image: asserts/k8s-sidecar:1.14.2
        imagePullPolicy: IfNotPresent
        env:
          - name: LABEL
            value: bootstrap-relabel-config
          - name: FOLDER
            value: /etc/asserts/relabel
          - name: METHOD
            value: LIST
        volumeMounts:
          - name: relabel-config
            mountPath: /etc/asserts/relabel

    extraArgs:
      loggerFormat: default
      relabelConfig: /etc/asserts/relabel/bootstrap.yml
      maxLabelsPerTimeseries: 60
      retentionPeriod: 30d
      search.maxStalenessInterval: 60s
      search.latencyOffset: 15s
      memory.allowedPercent: 50

    ## scrape configuration for the tsdb in config/tsdb-scrape-configmap.yaml
    scrape:
      enabled: true
      configMap: asserts-tsdb-scrapeconfig

    persistentVolume:
      size: 8Gi

    resources: {}

    extraContainers:
      ## TODO: config-sidecar takes care of hot loading relabeling rules
      ##       at the moment, the scrapeconfig is not currently hot loaded.
      ##       The scrapeconfig shouldn't need to be overridden in the values
      ##       file but can be reloaded by triggering the the REQ_URL below
      - name: config-sidecar
        image: asserts/k8s-sidecar:1.14.2
        imagePullPolicy: IfNotPresent
        env:
          - name: LABEL
            value: bootstrap-relabel-config
          - name: FOLDER
            value: /etc/asserts/relabel
          - name: REQ_URL
            value: http://localhost:8428/-/reload
        volumeMounts:
          - name: relabel-config
            mountPath: /etc/asserts/relabel

    extraVolumeMounts:
      - name: relabel-config
        mountPath: /etc/asserts/relabel

    extraVolumes:
      - name: relabel-config
        emptyDir: {}


## RedisGraph configuration
## ref: https://github.com/bitnami/charts/blob/master/bitnami/redis/values.yaml
redisgraph:
  enabled: true

  ## custom Redis multi module image
  ## ref: https://hub.docker.com/repository/docker/asserts/redismod
  image:
    repository: asserts/redismod
    tag: 6.26

  nameOverride: asserts-redisgraph
  fullnameOverride: asserts-redisgraph

  serviceAccount:
    create: false
    name: asserts

  architecture: standalone

  auth:
    enabled: false

  master:
    configuration: |
      loadmodule /opt/bitnami/redis/modules/redisgraph.so

  ## TODO: since we are scraping from the tsdb, may want to disable
  ##       the annotation -> prometheus.io/scrape: true
  ##       so that there isn't a double scrape from another prom
  metrics:
    enabled: true


## RedisSearch configuration
## ref: https://github.com/bitnami/charts/blob/master/bitnami/redis/values.yaml
redisearch:
  enabled: true

  ## custom Redis multi module image
  ## ref: https://hub.docker.com/repository/docker/asserts/redismod
  image:
    repository: asserts/redismod
    tag: 6.26

  nameOverride: asserts-redisearch
  fullnameOverride: asserts-redisearch

  serviceAccount:
    create: false
    name: asserts

  architecture: standalone

  auth:
    enabled: false

  master:
    configuration: |
      loadmodule /opt/bitnami/redis/modules/redisearch.so

  # TODO: since we are scraping from the tsdb, may want to disable
  #       the annotation -> prometheus.io/scrape: true
  #       so that there isn't a double scrape from another prom
  metrics:
    enabled: true


## Alertmanager configuration
## ref: https://github.com/asserts/helm-charts/blob/master/charts/alertmanager/values.yaml
alertmanager:
  enabled: true

  serviceAccountName: asserts
  serviceAccount:
    create: false
    name: asserts

  persistence:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 100Mi

  existingConfigMap: asserts-alertmanager

  configmapReload:
    enabled: true

## Promxy configuration
## ref: https://github.com/asserts/helm-charts/blob/master/charts/promxy/values.yaml
promxy:
  enabled: true
  serviceAccount:
    create: false
    name: asserts

  existingConfigMap: asserts-promxy

  initContainers:
    - name: wait-for-tsdb
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Release.Name}}-tsdb-server.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8428"
        - "-t"
        - "300"

  extraContainers:
    - name: config-sidecar
      image: asserts/k8s-sidecar:1.14.2
      imagePullPolicy: IfNotPresent
      env:
        - name: LABEL
          value: bootstrap-rules-config
        - name: FOLDER
          value: /etc/asserts/rules
        - name: REQ_URL
          value: http://localhost:8082/-/reload
        - name: REQ_METHOD
          value: POST
      volumeMounts:
        - name: rules-config
          mountPath: /etc/asserts/rules

  extraVolumeMounts:
    - name: rules-config
      mountPath: /etc/asserts/rules

  extraVolumes:
    - name: rules-config
      emptyDir: {}


## Postgres configuration
## ref: https://github.com/bitnami/charts/blob/master/bitnami/postgresql/values.yaml
postgres:
  enabled: true

  ## Postgres password configuration
  ##
  ## If left with password: "" and existingSecret: ""
  ## a random alpha numeric password will be generated.
  ## Upon upgrading the release, you will need to provide
  ## the password as a value in subsequent updates.
  ## e.g.
  # postgres:
  #   global:
  #     postgresql:
  #       auth:
  #         postgresPassword: <postgres-password>
  ##
  ## If an existingSecret is created (recommended for production)
  ## then that will be used.
  ##
  ## ref: https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues/#credential-errors-while-upgrading-chart-releases
  global:
    postgresql:
      auth:
        database: "asserts"

  image:
    repository: bitnami/postgresql
    tag: 12.7.0

  nameOverride: "asserts-postgres"
  fullnameOverride: "asserts-postgres"

  serviceAccount:
    create: false
    name: asserts

  primary:
    initdb:
      scripts:
        load-extensions.sh: |
          #!/bin/sh

          psql --username "postgres" <<EOF
          create extension IF NOT EXISTS pg_stat_statements;
          select * FROM pg_extension;
          EOF

  metrics:
    enabled: true
    ## TODO: figure out warning on install of chart:
    ## "warning: cannot overwrite table with non table for customMetrics (map[])""
    # customMetrics: |
    #   pg_replication:
    #     query: "SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag"
    #     master: true
    #     metrics:
    #       - lag:
    #           usage: "GAUGE"
    #           description: "Replication lag behind master in seconds"

    #   pg_postmaster:
    #     query: "SELECT pg_postmaster_start_time as start_time_seconds from pg_postmaster_start_time()"
    #     master: true
    #     metrics:
    #       - start_time_seconds:
    #           usage: "GAUGE"
    #           description: "Time at which postmaster started"

    #   pg_stat_user_tables:
    #     query: "SELECT current_database() datname, schemaname, relname, seq_scan, seq_tup_read, idx_scan, idx_tup_fetch, n_tup_ins, n_tup_upd, n_tup_del, n_tup_hot_upd, n_live_tup, n_dead_tup, n_mod_since_analyze, COALESCE(last_vacuum, '1970-01-01Z'), COALESCE(last_vacuum, '1970-01-01Z') as last_vacuum, COALESCE(last_autovacuum, '1970-01-01Z') as last_autovacuum, COALESCE(last_analyze, '1970-01-01Z') as last_analyze, COALESCE(last_autoanalyze, '1970-01-01Z') as last_autoanalyze, vacuum_count, autovacuum_count, analyze_count, autoanalyze_count FROM pg_stat_user_tables"
    #     metrics:
    #       - datname:
    #           usage: "LABEL"
    #           description: "Name of current database"
    #       - schemaname:
    #           usage: "LABEL"
    #           description: "Name of the schema that this table is in"
    #       - relname:
    #           usage: "LABEL"
    #           description: "Name of this table"
    #       - seq_scan:
    #           usage: "COUNTER"
    #           description: "Number of sequential scans initiated on this table"
    #       - seq_tup_read:
    #           usage: "COUNTER"
    #           description: "Number of live rows fetched by sequential scans"
    #       - idx_scan:
    #           usage: "COUNTER"
    #           description: "Number of index scans initiated on this table"
    #       - idx_tup_fetch:
    #           usage: "COUNTER"
    #           description: "Number of live rows fetched by index scans"
    #       - n_tup_ins:
    #           usage: "COUNTER"
    #           description: "Number of rows inserted"
    #       - n_tup_upd:
    #           usage: "COUNTER"
    #           description: "Number of rows updated"
    #       - n_tup_del:
    #           usage: "COUNTER"
    #           description: "Number of rows deleted"
    #       - n_tup_hot_upd:
    #           usage: "COUNTER"
    #           description: "Number of rows HOT updated (i.e., with no separate index update required)"
    #       - n_live_tup:
    #           usage: "GAUGE"
    #           description: "Estimated number of live rows"
    #       - n_dead_tup:
    #           usage: "GAUGE"
    #           description: "Estimated number of dead rows"
    #       - n_mod_since_analyze:
    #           usage: "GAUGE"
    #           description: "Estimated number of rows changed since last analyze"
    #       - last_vacuum:
    #           usage: "GAUGE"
    #           description: "Last time at which this table was manually vacuumed (not counting VACUUM FULL)"
    #       - last_autovacuum:
    #           usage: "GAUGE"
    #           description: "Last time at which this table was vacuumed by the autovacuum daemon"
    #       - last_analyze:
    #           usage: "GAUGE"
    #           description: "Last time at which this table was manually analyzed"
    #       - last_autoanalyze:
    #           usage: "GAUGE"
    #           description: "Last time at which this table was analyzed by the autovacuum daemon"
    #       - vacuum_count:
    #           usage: "COUNTER"
    #           description: "Number of times this table has been manually vacuumed (not counting VACUUM FULL)"
    #       - autovacuum_count:
    #           usage: "COUNTER"
    #           description: "Number of times this table has been vacuumed by the autovacuum daemon"
    #       - analyze_count:
    #           usage: "COUNTER"
    #           description: "Number of times this table has been manually analyzed"
    #       - autoanalyze_count:
    #           usage: "COUNTER"
    #           description: "Number of times this table has been analyzed by the autovacuum daemon"

    #   pg_statio_user_tables:
    #     query: "SELECT current_database() datname, schemaname, relname, heap_blks_read, heap_blks_hit, idx_blks_read, idx_blks_hit, toast_blks_read, toast_blks_hit, tidx_blks_read, tidx_blks_hit FROM pg_statio_user_tables"
    #     metrics:
    #       - datname:
    #           usage: "LABEL"
    #           description: "Name of current database"
    #       - schemaname:
    #           usage: "LABEL"
    #           description: "Name of the schema that this table is in"
    #       - relname:
    #           usage: "LABEL"
    #           description: "Name of this table"
    #       - heap_blks_read:
    #           usage: "COUNTER"
    #           description: "Number of disk blocks read from this table"
    #       - heap_blks_hit:
    #           usage: "COUNTER"
    #           description: "Number of buffer hits in this table"
    #       - idx_blks_read:
    #           usage: "COUNTER"
    #           description: "Number of disk blocks read from all indexes on this table"
    #       - idx_blks_hit:
    #           usage: "COUNTER"
    #           description: "Number of buffer hits in all indexes on this table"
    #       - toast_blks_read:
    #           usage: "COUNTER"
    #           description: "Number of disk blocks read from this table's TOAST table (if any)"
    #       - toast_blks_hit:
    #           usage: "COUNTER"
    #           description: "Number of buffer hits in this table's TOAST table (if any)"
    #       - tidx_blks_read:
    #           usage: "COUNTER"
    #           description: "Number of disk blocks read from this table's TOAST table indexes (if any)"
    #       - tidx_blks_hit:
    #           usage: "COUNTER"
    #           description: "Number of buffer hits in this table's TOAST table indexes (if any)"

    #   pg_database:
    #     query: "SELECT pg_database.datname, pg_database_size(pg_database.datname) as size FROM pg_database"
    #     master: true
    #     cache_seconds: 30
    #     metrics:
    #       - datname:
    #           usage: "LABEL"
    #           description: "Name of the database"
    #       - size_bytes:
    #           usage: "GAUGE"
    #           description: "Disk space used by the database"

    #   pg_stat_statements:
    #     query: "SELECT t2.rolname, t3.datname, queryid, calls, total_time / 1000 as total_time_seconds, min_time / 1000 as min_time_seconds, max_time / 1000 as max_time_seconds, mean_time / 1000 as mean_time_seconds, stddev_time / 1000 as stddev_time_seconds, rows, shared_blks_hit, shared_blks_read, shared_blks_dirtied, shared_blks_written, local_blks_hit, local_blks_read, local_blks_dirtied, local_blks_written, temp_blks_read, temp_blks_written, blk_read_time / 1000 as blk_read_time_seconds, blk_write_time / 1000 as blk_write_time_seconds FROM pg_stat_statements t1 join pg_roles t2 on (t1.userid=t2.oid) join pg_database t3 on (t1.dbid=t3.oid)"
    #     master: true
    #     metrics:
    #       - rolname:
    #           usage: "LABEL"
    #           description: "Name of user"
    #       - datname:
    #           usage: "LABEL"
    #           description: "Name of database"
    #       - queryid:
    #           usage: "LABEL"
    #           description: "Query ID"
    #       - calls:
    #           usage: "COUNTER"
    #           description: "Number of times executed"
    #       - total_time_seconds:
    #           usage: "COUNTER"
    #           description: "Total time spent in the statement, in milliseconds"
    #       - min_time_seconds:
    #           usage: "GAUGE"
    #           description: "Minimum time spent in the statement, in milliseconds"
    #       - max_time_seconds:
    #           usage: "GAUGE"
    #           description: "Maximum time spent in the statement, in milliseconds"
    #       - mean_time_seconds:
    #           usage: "GAUGE"
    #           description: "Mean time spent in the statement, in milliseconds"
    #       - stddev_time_seconds:
    #           usage: "GAUGE"
    #           description: "Population standard deviation of time spent in the statement, in milliseconds"
    #       - rows:
    #           usage: "COUNTER"
    #           description: "Total number of rows retrieved or affected by the statement"
    #       - shared_blks_hit:
    #           usage: "COUNTER"
    #           description: "Total number of shared block cache hits by the statement"
    #       - shared_blks_read:
    #           usage: "COUNTER"
    #           description: "Total number of shared blocks read by the statement"
    #       - shared_blks_dirtied:
    #           usage: "COUNTER"
    #           description: "Total number of shared blocks dirtied by the statement"
    #       - shared_blks_written:
    #           usage: "COUNTER"
    #           description: "Total number of shared blocks written by the statement"
    #       - local_blks_hit:
    #           usage: "COUNTER"
    #           description: "Total number of local block cache hits by the statement"
    #       - local_blks_read:
    #           usage: "COUNTER"
    #           description: "Total number of local blocks read by the statement"
    #       - local_blks_dirtied:
    #           usage: "COUNTER"
    #           description: "Total number of local blocks dirtied by the statement"
    #       - local_blks_written:
    #           usage: "COUNTER"
    #           description: "Total number of local blocks written by the statement"
    #       - temp_blks_read:
    #           usage: "COUNTER"
    #           description: "Total number of temp blocks read by the statement"
    #       - temp_blks_written:
    #           usage: "COUNTER"
    #           description: "Total number of temp blocks written by the statement"
    #       - blk_read_time_seconds:
    #           usage: "COUNTER"
    #           description: "Total time the statement spent reading blocks, in milliseconds (if track_io_timing is enabled, otherwise zero)"
    #       - blk_write_time_seconds:
    #           usage: "COUNTER"
    #           description: "Total time the statement spent writing blocks, in milliseconds (if track_io_timing is enabled, otherwise zero)"
